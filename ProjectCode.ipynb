{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9wUEcL8Ys0j4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Niller\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Niller\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cleantext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP3Hzp-2s3nn"
      },
      "outputs": [],
      "source": [
        "#Storage: \n",
        "# define regular expressions for cleaning\n",
        "num_re = re.compile(r\"\\d+\")\n",
        "# Formatting dates with YYYY-MM-DD since that's how it's formatted in the columns of the data.\n",
        "date_re = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n",
        "email_re = re.compile(r\"\\S+@\\S+\")\n",
        "url_re = re.compile(r\"https?://\\S+\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "tokenlist = []\n",
        "stopwordlist = []\n",
        "stemmedlist = []\n",
        "vocabulary_size = []\n",
        "filtered_vocabulary_size = []\n",
        "stemmed_vocabulary_size = []\n",
        "def clean_text(text):\n",
        "    # lower case all words\n",
        "    text = text.lower()\n",
        "    # replace multiple white spaces, tabs, or new lines with a single space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    # replace numbers, dates, emails, and URLs with special tokens\n",
        "    text = date_re.sub(\"<DATE>\", text)\n",
        "    text = email_re.sub(\"<EMAIL>\", text)\n",
        "    text = url_re.sub(\"<URL>\", text)\n",
        "    text = num_re.sub(\"<NUM>\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    # Removing tokens that come from tags\n",
        "    tags = ['<','>','num','date','email','url']\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.lower() not in tags and re.match(r'^\\w', token):\n",
        "            filtered_tokens.append(token)\n",
        "    # Removing stopwords\n",
        "    tokenlist.append(filtered_tokens)\n",
        "    stopwordless_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() not in stop_words:\n",
        "            stopwordless_tokens.append(token)\n",
        "    stopwordlist.append(stopwordless_tokens)\n",
        "    # Perform stemming\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in stopwordless_tokens]\n",
        "    stemmedlist.append(stemmed_tokens)\n",
        "    # Compute vocabulary size before and after removing stopwords and stemming\n",
        "    vocabulary_size.append(len(set(filtered_tokens)))\n",
        "    filtered_vocabulary_size.append(len(set(stopwordless_tokens)))\n",
        "    stemmed_vocabulary_size.append(len(set(stemmed_tokens)))\n",
        "    stemmedtext = ' '.join(stemmed_tokens)\n",
        "    return (stemmedtext)\n",
        "#vocab_sum = sum(vocabulary_size)\n",
        "#filtered_sum= sum(filtered_vocabulary_size)\n",
        "#stemmed_sum = sum(stemmed_vocabulary_size)\n",
        "#print(\"Vocabulary size:\", vocab_sum ) \n",
        "#print(\"Vocabulary size without stopwords:\",  filtered_sum, \", resulting in reduction rate:\", filtered_sum/vocab_sum)\n",
        "#print(\"Vocabulary size after stemming with no stopwords:\", stemmed_sum, \", resulting in reduction rate:\", stemmed_sum/filtered_sum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qZO9qt-xs4kO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: <bound method NDFrame._add_numeric_operations.<locals>.sum of 0       505\n",
            "1       176\n",
            "2       451\n",
            "3       504\n",
            "4       283\n",
            "       ... \n",
            "245    1292\n",
            "246     574\n",
            "247     220\n",
            "248     424\n",
            "249     111\n",
            "Name: content, Length: 250, dtype: int64>\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for /: 'method' and 'method'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m stemmed_sum \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(count_words)\u001b[39m.\u001b[39msum\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mVocabulary size:\u001b[39m\u001b[39m\"\u001b[39m, vocab_sum ) \n\u001b[1;32m---> 24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mVocabulary size without stopwords:\u001b[39m\u001b[39m\"\u001b[39m,  filtered_sum, \u001b[39m\"\u001b[39m\u001b[39m, resulting in reduction rate:\u001b[39m\u001b[39m\"\u001b[39m, filtered_sum\u001b[39m/\u001b[39;49mvocab_sum)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mVocabulary size after stemming with no stopwords:\u001b[39m\u001b[39m\"\u001b[39m, stemmed_sum, \u001b[39m\"\u001b[39m\u001b[39m, resulting in reduction rate:\u001b[39m\u001b[39m\"\u001b[39m, stemmed_sum\u001b[39m/\u001b[39mfiltered_sum)\n",
            "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'method' and 'method'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('news_sample.csv')\n",
        "def clean_module(text):\n",
        "    return cleantext.clean(text, clean_all= False, # Execute all cleaning operations\n",
        "extra_spaces=True ,  # Remove extra white spaces \n",
        "lowercase=True ,# Convert to lowercase\n",
        "numbers=True ,# Remove all digits \n",
        "punct=True ,# Remove all punctuations\n",
        "stp_lang='english'\n",
        "  # Language for stop words\n",
        ")\n",
        "def rem_stopwords(text):\n",
        "    return cleantext.clean(text,stopwords=True)\n",
        "def stem(text):\n",
        "  return cleantext.clean(text,stemming=True)\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "df['content'] = df['content'].apply(clean_module)\n",
        "vocab_sum = df['content'].apply(count_words).sum\n",
        "df['content'] = df['content'].apply(rem_stopwords)\n",
        "filtered_sum= df['content'].apply(count_words).sum\n",
        "df['content'] = df['content'].apply(stem)\n",
        "stemmed_sum = df['content'].apply(count_words).sum\n",
        "print(\"Vocabulary size:\", vocab_sum ) \n",
        "print(\"Vocabulary size without stopwords:\",  filtered_sum, \", resulting in reduction rate:\", filtered_sum/vocab_sum)\n",
        "print(\"Vocabulary size after stemming with no stopwords:\", stemmed_sum, \", resulting in reduction rate:\", stemmed_sum/filtered_sum)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi0mmkGHs9eh"
      },
      "source": [
        "Observations: \n",
        "\n",
        "  1.All articles from www.newsmax.com, willyloman.wordpress and wallstreetonparade.com do not have a type.\n",
        "\n",
        "2. The ids go up with the article order\n",
        "\n",
        "3. The average amount of stemmed words change with each type, this is however a small sample size for a lot of the types.\n",
        "\n",
        "4. The count of types vary a lot, 62% are \"fake\", yet only 1/250 is \"clickbait\" or \"hate\".\n",
        "\n",
        "5. The words a more evenly spread without stopwords (see graphs below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTxeySiJs-ca"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "# Use Counter to count the frequency of each word\n",
        "word_counts = Counter(sum(tokenlist, []))\n",
        "\n",
        "# Get the 50 most common words\n",
        "top_words = word_counts.most_common(50)\n",
        "\n",
        "# Separate out the words and their counts\n",
        "word_labels = [word[0] for word in top_words]\n",
        "word_freqs = [word[1] for word in top_words]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Create the first subplot\n",
        "plt.subplots(figsize=(25, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Top 50 words before processing')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.bar(word_labels, word_freqs)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Create the second subplot\n",
        "word_counts = Counter(sum(stopwordlist, []))\n",
        "top_words = word_counts.most_common(50)\n",
        "word_labels = [word[0] for word in top_words]\n",
        "word_freqs = [word[1] for word in top_words]\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Top 50 words after removing stopwords')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.bar(word_labels, word_freqs)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Show the figure\n",
        "plt.subplots_adjust(wspace=0.15) # adjust the width space between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0gtuNOms_oU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set the file path\n",
        "file_path = 'news_cleaned_2018_02_13.csv'\n",
        "\n",
        "# Determine the number of rows to skip based on the 10% sampling rate\n",
        "num_rows = sum(1 for line in open(file_path, encoding='utf-8')) // 10\n",
        "\n",
        "# Load the data, skipping every 10th row\n",
        "dff = pd.read_csv(file_path, skiprows=lambda i: i % 10 != 0, encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNRGu9SwtAht"
      },
      "outputs": [],
      "source": [
        "def clean_module(text):\n",
        "    text = date_re.sub(\"<DATE>\", text)\n",
        "    return cleantext.clean(text, clean_all= False, # Execute all cleaning operations\n",
        "extra_spaces=True ,  # Remove extra white spaces \n",
        "stemming=True , # Stem the words\n",
        "stopwords=True ,# Remove stop words\n",
        "lowercase=True ,# Convert to lowercase\n",
        "numbers=True ,# Remove all digits \n",
        "punct=True ,# Remove all punctuations\n",
        "stp_lang='english'\n",
        "  # Language for stop words\n",
        ")\n",
        "# define regular expressions for cleaning\n",
        "num_re = re.compile(r\"\\d+\")\n",
        "# Formatting dates with YYYY-MM-DD since that's how it's formatted in the columns of the data.\n",
        "date_re = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n",
        "email_re = re.compile(r\"\\S+@\\S+\")\n",
        "url_re = re.compile(r\"https?://\\S+\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "tokenlist = []\n",
        "stopwordlist = []\n",
        "stemmedlist = []\n",
        "vocabulary_size = []\n",
        "filtered_vocabulary_size = []\n",
        "stemmed_vocabulary_size = []\n",
        "def clean_text(text):\n",
        "    # lower case all words\n",
        "    text = text.lower()\n",
        "    # replace multiple white spaces, tabs, or new lines with a single space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    # replace numbers, dates, emails, and URLs with special tokens\n",
        "    text = date_re.sub(\"<DATE>\", text)\n",
        "    text = email_re.sub(\"<EMAIL>\", text)\n",
        "    text = url_re.sub(\"<URL>\", text)\n",
        "    text = num_re.sub(\"<NUM>\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    # Removing tokens that come from tags\n",
        "    tags = ['<','>','num','date','email','url']\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.lower() not in tags and re.match(r'^\\w', token):\n",
        "            filtered_tokens.append(token)\n",
        "    # Removing stopwords\n",
        "    tokenlist.append(filtered_tokens)\n",
        "    stopwordless_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() not in stop_words:\n",
        "            stopwordless_tokens.append(token)\n",
        "    stopwordlist.append(stopwordless_tokens)\n",
        "    # Perform stemming\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in stopwordless_tokens]\n",
        "    stemmedlist.append(stemmed_tokens)\n",
        "    # Compute vocabulary size before and after removing stopwords and stemming\n",
        "    vocabulary_size.append(len(set(filtered_tokens)))\n",
        "    filtered_vocabulary_size.append(len(set(stopwordless_tokens)))\n",
        "    stemmed_vocabulary_size.append(len(set(stemmed_tokens)))\n",
        "    stemmedtext = ' '.join(stemmed_tokens)\n",
        "    return (stemmedtext)\n",
        "\n",
        "dff = dff.loc[dff['content'].apply(lambda x: not isinstance(x, float))]\n",
        "dff['content'] = dff['content'].apply(clean_module)\n",
        "#vocab_sum = sum(vocabulary_size)\n",
        "#filtered_sum= sum(filtered_vocabulary_size)\n",
        "#stemmed_sum = sum(stemmed_vocabulary_size)\n",
        "#print(\"Vocabulary size:\", vocab_sum ) \n",
        "#print(\"Vocabulary size without stopwords:\",  filtered_sum, \", resulting in reduction rate:\", filtered_sum/vocab_sum)\n",
        "#print(\"Vocabulary size after stemming with no stopwords:\", stemmed_sum, \", resulting in reduction rate:\", stemmed_sum/filtered_sum)\n",
        "\n",
        "print(dff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGpvTUKctCfl"
      },
      "outputs": [],
      "source": [
        "df.to_csv('10percentcleandata.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6Lld00QNtEFZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Niller\\AppData\\Local\\Temp\\ipykernel_6028\\876483081.py:1: DtypeWarning: Columns (0,1,11,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('10percentcleandata.csv')\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('10percentcleandata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukywI1BVtHyA"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_values = df['type'].unique()\n",
        "\n",
        "# print the unique values\n",
        "print(unique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO-Spf-ltInY"
      },
      "outputs": [],
      "source": [
        "# create a dictionary to map the values to \"fake\" or \"true\"\n",
        "mapping = {'rumor': 'fake',\n",
        "           'hate': 'fake',\n",
        "           'unreliable': 'fake',\n",
        "           'conspiracy': 'fake',\n",
        "           'clickbait': 'fake',\n",
        "           'satire': 'fake',\n",
        "           'bias': 'fake',\n",
        "           'junksci': 'fake',\n",
        "           'political': 'reliable'}\n",
        "\n",
        "# replace the values using the mapping dictionary\n",
        "df['type'] = df['type'].replace(mapping)\n",
        "# filter out the rows where 'Column1' is not equal to either \"true\" or \"fake\"\n",
        "df = df[(df['type'] == 'reliable') | (df['type'] == 'fake')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJAOwfWbtJtd"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_values = df['type'].unique()\n",
        "\n",
        "# print the unique values\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztq1-mq3tKpa"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Create a CountVectorizer object to transform the documents into a bag of words representation\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents and transform them into a matrix of word counts\n",
        "wordvector = vectorizer.fit_transform(df['content'])\n",
        "\n",
        "# Print the vocabulary of the vectorizer (the unique words in the corpus)\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(wordvector, df['type'], test_size=0.2)\n",
        "\n",
        "# Train a logistic regression classifier on the training data\n",
        "clf = LogisticRegression()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier on the test data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9FU2eB7tLhE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Create a CountVectorizer object to transform the documents into a bag of words representation\n",
        "vectorizer = CountVectorizer()\n",
        "# Create a new column in the dataframe for the domain information\n",
        "#df['domain_info'] = df['domain'].astype(str)\n",
        "\n",
        "# Concatenate the domain information with the content of each document\n",
        "df['content'] = df['domain'] + df['content']\n",
        "# Fit the vectorizer to the documents and transform them into a matrix of word counts\n",
        "wordvector = vectorizer.fit_transform(df['content'])\n",
        "\n",
        "# Print the vocabulary of the vectorizer (the unique words in the corpus)\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(wordvector, df['type'], test_size=0.2)\n",
        "\n",
        "# Train a logistic regression classifier on the training data\n",
        "clf = LogisticRegression()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier on the test data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrCdPHTltNcG"
      },
      "outputs": [],
      "source": [
        "#Data for advanced model\n",
        "df = pd.read_csv('10percentcleandata.csv')\n",
        "mapping = {'rumor': 'unreliable',\n",
        "           'hate': 'fake',\n",
        "           'unreliable': 'unreliable',\n",
        "           'conspiracy': 'unreliable',\n",
        "           'clickbait': 'unreliable',\n",
        "           'satire': 'satire',\n",
        "           'bias': 'unreliable',\n",
        "           'junksci': 'fake',}\n",
        "df = df[(df['type'] == 'reliable') | (df['type'] == 'fake')| (df['type'] == 'hate')| (df['type'] == 'satire')| (df['type'] == 'clickbait')| (df['type'] == 'rumor')]\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['type'], test_size=0.2)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We chose to split our articles into 4 types: \"unreliable\" (composed of \"rumor\", \"conspiracy\", \"clickbait\", \"bias\" and ofc itself), \"fake\" which is composed of \"hate\",\"junksci\" and itself, \"satire\" which is only composed of itself and \"reliable\" which is only composed of itself. We chose to eliminate all the articles flagged as \"political\" as we don't see it fit in with any of the other categories since it by itself doesn't say anything about the truthfullness of the article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBRouuDctNyF"
      },
      "outputs": [],
      "source": [
        "#Advanced model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "X = tfidf.fit_transform(df['content'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# Convert the text content into numerical data using TfidfVectorizer\u001b[39;00m\n\u001b[0;32m     12\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(max_features\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(df[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     15\u001b[0m \u001b[39m# Encode the article type column as categorical variables\u001b[39;00m\n\u001b[0;32m     16\u001b[0m encoder \u001b[39m=\u001b[39m LabelEncoder()\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:118\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    116\u001b[0m             doc \u001b[39m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    117\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m             doc \u001b[39m=\u001b[39m ngrams(doc)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:245\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m         )\n\u001b[0;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n\u001b[1;32m--> 245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_word_ngrams\u001b[39m(\u001b[39mself\u001b[39m, tokens, stop_words\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    246\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[39m# handle stop words\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# Convert the text content into numerical data using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X = vectorizer.fit_transform(df['content'])\n",
        "\n",
        "# Encode the article type column as categorical variables\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['type'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the layers of the model\n",
        "input_layer = Input(shape=(X_train.shape[1],))\n",
        "hidden_layer = Dense(64, activation='relu')(input_layer)\n",
        "output_layer = Dense(len(encoder.classes_), activation='softmax')(hidden_layer)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
