{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wUEcL8Ys0j4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cleantext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZO9qt-xs4kO"
      },
      "outputs": [],
      "source": [
        "# Reading the sample set\n",
        "df = pd.read_csv('news_sample.csv')\n",
        "\n",
        "# Defining our text cleaning function for news sample\n",
        "def clean_module(text):\n",
        "    return cleantext.clean(text, clean_all= False, # Execute all cleaning operations\n",
        "    extra_spaces=True ,# Remove extra white spaces \n",
        "    lowercase=True ,# Convert to lowercase\n",
        "    numbers=True ,# Remove all digits \n",
        "    punct=True ,# Remove all punctuations\n",
        "    reg=r'[^\\w\\s]'\n",
        "    )\n",
        "\n",
        "# Defining function to remove stopwords and stemming\n",
        "def rem_stopwords(text):\n",
        "    return cleantext.clean(text,stopwords=True)\n",
        "def stem(text):\n",
        "  return cleantext.clean(text,stemming=True)\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# Cleaning the \"content\" column using our function and tokenizing it\n",
        "df['content'] = df['content'].apply(clean_module)\n",
        "vocab_sum = df['content'].str.split().apply(len).sum()\n",
        "tokenlist = df['content'].apply(word_tokenize).tolist()\n",
        "# Removing stopwords\n",
        "df['content'] = df['content'].apply(rem_stopwords)\n",
        "filtered_sum= df['content'].str.split().apply(len).sum()\n",
        "stopwordlist = df['content'].apply(word_tokenize).tolist()\n",
        "# Stemming\n",
        "df['content'] = df['content'].apply(stem)\n",
        "stemmed_sum = df['content'].str.split().apply(len).sum()\n",
        "# Calculating reduction rate when stopwords have been removed and when stemming\n",
        "print(\"Vocabulary size:\", vocab_sum ) \n",
        "print(\"Vocabulary size without stopwords:\",  filtered_sum, \", resulting in reduction rate:\", filtered_sum/vocab_sum)\n",
        "print(\"Vocabulary size after stemming with no stopwords:\", stemmed_sum, \", resulting in reduction rate:\", stemmed_sum/filtered_sum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTxeySiJs-ca"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "# Using Counter to count the frequency of each word\n",
        "word_counts = Counter(sum(tokenlist, []))\n",
        "\n",
        "# Get the 50 most common words\n",
        "top_words = word_counts.most_common(50)\n",
        "\n",
        "# Separate out the words and their counts\n",
        "word_labels = [word[0] for word in top_words]\n",
        "word_freqs = [word[1] for word in top_words]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Create the first subplot\n",
        "plt.subplots(figsize=(25, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Top 50 words before processing')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.bar(word_labels, word_freqs)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Create the second subplot\n",
        "word_counts = Counter(sum(stopwordlist, []))\n",
        "top_words = word_counts.most_common(50)\n",
        "word_labels = [word[0] for word in top_words]\n",
        "word_freqs = [word[1] for word in top_words]\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Top 50 words after removing stopwords')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.bar(word_labels, word_freqs)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Show the figure\n",
        "plt.subplots_adjust(wspace=0.15) # adjust the width space between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0gtuNOms_oU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Loading the Fakenews dataset and extracting 10%\n",
        "\n",
        "# Set the file path\n",
        "file_path = 'news_cleaned_2018_02_13.csv'\n",
        "\n",
        "# Determine the number of rows to skip based on the 10% sampling rate\n",
        "num_rows = sum(1 for line in open(file_path, encoding='utf-8')) // 10\n",
        "\n",
        "# Load the data, skipping every 10th row\n",
        "dff = pd.read_csv(file_path, skiprows=lambda i: i % 10 != 0, encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNRGu9SwtAht"
      },
      "outputs": [],
      "source": [
        "# Defining new cleaning function for the big dataset\n",
        "def clean_module(text):\n",
        "    return cleantext.clean(text, clean_all= False,\n",
        "extra_spaces=True , \n",
        "stemming=True , \n",
        "stopwords=True ,\n",
        "lowercase=True ,\n",
        "numbers=True , \n",
        "punct=True ,\n",
        "reg=r'[^\\w\\s]',\n",
        "stp_lang='english'\n",
        ")\n",
        "\n",
        "# Cleaning the reduced 10% dataset\n",
        "dff = dff.loc[dff['content'].apply(lambda x: not isinstance(x, float))] #Removes rows where content is a float to avoid errors\n",
        "dff['content'] = dff['content'].apply(clean_module)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGpvTUKctCfl"
      },
      "outputs": [],
      "source": [
        "# Saving the reduced dataset to avoid the long reading and cleaning process again\n",
        "df.to_csv('10percentcleandata.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lld00QNtEFZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('10percentcleandata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukywI1BVtHyA"
      },
      "outputs": [],
      "source": [
        "unique_values = df['type'].unique()\n",
        "\n",
        "# printing the unique values of labels\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO-Spf-ltInY"
      },
      "outputs": [],
      "source": [
        "# create a dictionary to map the values to \"fake\" or \"reliable\"\n",
        "mapping = {'rumor': 'fake',\n",
        "           'hate': 'fake',\n",
        "           'unreliable': 'fake',\n",
        "           'conspiracy': 'fake',\n",
        "           'clickbait': 'fake',\n",
        "           'satire': 'fake',\n",
        "           'bias': 'fake',\n",
        "           'junksci': 'fake',\n",
        "           'political': 'reliable'}\n",
        "\n",
        "# replace the values using the mapping dictionary\n",
        "df['type'] = df['type'].replace(mapping)\n",
        "# filter out the rows where 'type' is not equal to either \"reliable\" or \"fake\"\n",
        "df = df[(df['type'] == 'reliable') | (df['type'] == 'fake')]\n",
        "# Sampling the dataset to avoid having only a few domains in the test set since they are ordered\n",
        "df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztq1-mq3tKpa"
      },
      "outputs": [],
      "source": [
        "# Simple model with only content\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a CountVectorizer object to transform the documents into a bag of words representation\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents and transform them into a matrix of word counts\n",
        "wordvector = vectorizer.fit_transform(df['content'])\n",
        "\n",
        "# Splitting the (reduced) fakenews dataset into training set and test set\n",
        "train_size = int(len(df) * .8)\n",
        "X_train = wordvector[:train_size]\n",
        "y_train = df['type'][:train_size]\n",
        "\n",
        "X_test = wordvector[train_size:]\n",
        "y_test = df['type'][train_size:]\n",
        "\n",
        "# Train a logistic regression model on the training data\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the labels for the fakenews test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculating accuracy and f1-score of the model on the fakenews test data\n",
        "simple_accuracy_fakenews = accuracy_score(y_test, y_pred)\n",
        "simple_fscore_fakenews = f1_score(y_test, y_pred, pos_label = \"reliable\")\n",
        "print(\"Accuracy: \", simple_accuracy_fakenews)\n",
        "print(\"F1-Score: \", simple_fscore_fakenews)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reading in the LIAR test set\n",
        "liar = pd.read_csv('test.tsv',sep='\\t')\n",
        "\n",
        "# Mapping\n",
        "mapping = {'false': 'fake',\n",
        "           'half-true': 'fake',\n",
        "           'pants-fire': 'fake',\n",
        "           'true': 'reliable',\n",
        "           'barely-true': 'fake',\n",
        "           'mostly-true': 'reliable'\n",
        "           }\n",
        "liar['true'] = liar['true'].replace(mapping)\n",
        "\n",
        "\n",
        "# Cleaning the LIAR test set with our cleaning function\n",
        "# We notice the LIAR test set has no headers hence the weird labels\n",
        "liar = liar.loc[liar['Building a wall on the U.S.-Mexico border will take literally years.'].apply(lambda x: not isinstance(x, float))]\n",
        "liar['Building a wall on the U.S.-Mexico border will take literally years.'] = liar['Building a wall on the U.S.-Mexico border will take literally years.'].apply(clean_module)\n",
        "\n",
        "# Using our simple model to predict labels\n",
        "X_test = vectorizer.transform(liar[\"Building a wall on the U.S.-Mexico border will take literally years.\"])\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculating accuracy and f1-score of the model on the LIAR test data\n",
        "simple_fscore_liar = f1_score(liar[\"true\"], y_pred, pos_label=\"reliable\")\n",
        "simple_accuracy_liar = accuracy_score(liar[\"true\"], y_pred)\n",
        "print(\"Accuracy:\", simple_accuracy_liar)\n",
        "print(\"F1-Score:\", simple_fscore_liar)\n",
        "print(confusion_matrix(liar[\"true\"], y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9FU2eB7tLhE"
      },
      "outputs": [],
      "source": [
        "# Simple model with Domain\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a CountVectorizer object to transform the documents into a bag of words representation\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Concatenate the domain information with the content of each document\n",
        "df['content'] = df['domain'] + df['content']\n",
        "# Fit the vectorizer to the documents and transform them into a matrix of word counts\n",
        "wordvector = vectorizer.fit_transform(df['content'])\n",
        "\n",
        "# Print the vocabulary of the vectorizer (the unique words in the corpus)\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "train_size = int(len(df) * .8)\n",
        "X_train = wordvector[:train_size]\n",
        "y_train = df['type'][:train_size]\n",
        "\n",
        "X_test = wordvector[train_size:]\n",
        "y_test = df['type'][train_size:]\n",
        "\n",
        "# Train a logistic regression classifier on the training data\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier on the test data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrCdPHTltNcG"
      },
      "outputs": [],
      "source": [
        "# Data for advanced model\n",
        "\n",
        "# Reading in our reduced cleaned fakenews dataset\n",
        "df = pd.read_csv('10percentcleandata.csv')\n",
        "\n",
        "# Mapping\n",
        "mapping = {'rumor': 'unreliable',\n",
        "           'hate': 'fake',\n",
        "           'unreliable': 'unreliable',\n",
        "           'conspiracy': 'unreliable',\n",
        "           'clickbait': 'unreliable',\n",
        "           'satire': 'satire',\n",
        "           'bias': 'unreliable',\n",
        "           'junksci': 'fake',}\n",
        "df['type'] = df['type'].replace(mapping)\n",
        "\n",
        "# Make sure only rows with the correct labels are present\n",
        "df = df[(df['type'] == 'reliable') | (df['type'] == 'fake')| (df['type'] == 'unreliable')| (df['type'] == 'satire')]\n",
        "# Sampling the dataset to avoid having only a few domains in the test set since they are ordered\n",
        "df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We chose to split our articles into 4 types: \"unreliable\" (composed of \"rumor\", \"conspiracy\", \"clickbait\", \"bias\" and of course itself), \"fake\" which is composed of \"hate\",\"junksci\" and itself, \"satire\" which is only composed of itself and \"reliable\" which is only composed of itself. We chose to eliminate all the articles flagged as \"political\" as we don't see it fit in with any of the other categories since it by itself doesn't say anything about the truthfullness of the article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finding the best NN parameters\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "\n",
        "# Splitting into training and test set\n",
        "train_size = int(len(df) * .8)\n",
        "train_content = df['content'][:train_size]\n",
        "train_type = df['type'][:train_size]\n",
        "test_content = df['content'][train_size:]\n",
        "test_type = df['type'][train_size:]\n",
        "\n",
        "# Defining a range of initial neuron values to try\n",
        "neuron_values = [128, 256, 512, 1024, 2048]\n",
        "\n",
        "# Set the other hyperparameters\n",
        "max_words = 2000\n",
        "batch_size = 1024\n",
        "epochs = 1\n",
        "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "tokenize.fit_on_texts(train_content)\n",
        "x_train = tokenize.texts_to_matrix(train_content)\n",
        "x_test = tokenize.texts_to_matrix(test_content)\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_type)\n",
        "y_train = encoder.transform(train_type)\n",
        "y_test = encoder.transform(test_type)\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Train and evaluate the models for different numbers of neurons\n",
        "for num_neurons in neuron_values:\n",
        "    print(f\"Training model with {num_neurons} neurons...\")\n",
        "    # Build the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_neurons, input_shape=(max_words,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        verbose=1,\n",
        "                        validation_split=0.1)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    score = model.evaluate(x_test, y_test,\n",
        "                           batch_size=batch_size,\n",
        "                           verbose=1)\n",
        "    print(f\"Test accuracy for {num_neurons} neurons: {score[1]}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the best model was with 2048 neurons so we'll train a model below with 2048 neurons that use a larger batch size and two epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full model training:\n",
        "batch_size = 500\n",
        "epochs = 2\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_split=0.1)\n",
        "    # Evaluate the model on the test set\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "test_predictions = model.predict(x_test)\n",
        "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_type)\n",
        "y_test = encoder.transform(test_type)\n",
        "# Calculate F1 score\n",
        "from sklearn.metrics import f1_score\n",
        "test_f1 = f1_score(y_test, test_predicted_labels, average='weighted')\n",
        "print(\"Test F1 score:\", test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Test on Liar set\n",
        "liar = pd.read_csv('test.tsv', sep=\"\\t\")\n",
        "def clean_module(text):\n",
        "    return cleantext.clean(text, clean_all= False,\n",
        "extra_spaces=True , \n",
        "stemming=True , \n",
        "stopwords=True ,\n",
        "lowercase=True ,\n",
        "numbers=True , \n",
        "punct=True ,\n",
        "stp_lang='english'\n",
        "  # Language for stop words\n",
        ")\n",
        "mapping = {'false': 'fake',\n",
        "           'half-true': 'unreliable',\n",
        "           'pants-fire': 'fake',\n",
        "           'true': 'reliable',\n",
        "           'barely-true': 'unreliable',\n",
        "           'mostly-true': 'reliable'\n",
        "           }\n",
        "liar['true'] = liar['true'].replace(mapping)\n",
        "\n",
        "# filter out the rows where 'Column1' is not equal to either \"reliable\" or \"fake\"\n",
        "liar = liar[(liar['true'] == 'reliable') | (liar['true'] == 'fake')| (liar['true'] == 'unreliable')| (liar['true'] == 'satire')]\n",
        "\n",
        "liar = liar.loc[liar['Building a wall on the U.S.-Mexico border will take literally years.'].apply(lambda x: not isinstance(x, float))]\n",
        "liar['Building a wall on the U.S.-Mexico border will take literally years.'] = liar['Building a wall on the U.S.-Mexico border will take literally years.'].apply(clean_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the text data\n",
        "new_content = liar['Building a wall on the U.S.-Mexico border will take literally years.']\n",
        "new_text = tokenize.texts_to_matrix(new_content)\n",
        "predictions = model.predict(new_text)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.array(liar['true'])\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_type)\n",
        "true_labels = encoder.transform(liar['true'])\n",
        "print(np.shape(true_labels))\n",
        "# Calculate F1 score\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(\"F1 score:\", f1)\n",
        "\n",
        "num_classes = np.max(true_labels) + 1\n",
        "y_test = utils.to_categorical(true_labels, num_classes)\n",
        "\n",
        "score = model.evaluate(new_text, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_mat = confusion_matrix(true_labels, predicted_labels)\n",
        "print(encoder.classes_)\n",
        "print(confusion_mat)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
